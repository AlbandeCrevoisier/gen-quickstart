{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Introduction to modeling in Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gen is a multi-paradigm platform for probabilistic modeling and inference. Gen supports multiple modeling and inference workflows, including:\n",
    "\n",
    "- Unsupervised learning and posterior inference in generative models using Monte Carlo,  variational, EM, and stochastic gradient techniques.\n",
    "\n",
    "- Supervised learning of conditional inference models (e.g. supervised classification and regression).\n",
    "\n",
    "- Hybrid approaches including amortized inference / inference compilation, variational autoencoders, and semi-supervised learning.\n",
    "\n",
    "In Gen, probabilistic models (both generative models and conditional inference models) are represented as _generative functions_. Gen provides a built-in modeling language for defining generative functions (Gen can also be extended to support other modeling languages, but this is not covered in this tutorial). This tutorial introduces the basics of Gen's built-in modeling language, and illustrates a few types of modeling flexibility afforded by the language, including:\n",
    "\n",
    "- Using a stochastic branching and function abstraction to express uncertainty about which of multiple models is appropriate.\n",
    "\n",
    "- Representing models with an unbounded number of parameters (a 'Bayesian non-parametric' model).\n",
    "\n",
    "- Using 'black-box' Julia code in models.\n",
    "\n",
    "- Using TensorFlow code in models.\n",
    "\n",
    "This notebook uses a simple generic inference algorithm for posterior inference. This tutorial does not cover *custom inference programming*, which is a key capability of Gen in which users implement inference algorithms that are specialized to their probabilistic model. Inference programming is important for getting accurate posterior inferences efficiently, and will be covered in later tutorials. Also, this tutorial does not exhaustively cover all features of the modeling language -- there are also features and extensions that provide improved performance that are not covered here.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Section 1.** [Julia, Gen, and this Jupyter notebook](#julia-gen-jupyter)\n",
    "\n",
    "**Section 2.** [Writing a probabilistic model as a generative function](#writing-model)\n",
    "\n",
    "**Section 3.** [Doing posterior inference](#doing-inference)\n",
    "\n",
    "**Section 4.** [Predicting new data](#predicting-data)\n",
    "\n",
    "**Section 5.** [Calling other generative functions](#calling-functions)\n",
    "\n",
    "**Section 6.** [Modeling with an unbounded number of parameters](#infinite-space)\n",
    "\n",
    "**Section 7.** [Modeling with black-box Julia code](#black-box)\n",
    "\n",
    "**Section 8.** [Modeling with TensorFlow code](#tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Julia, Gen, and this Jupyter notebook  <a name=\"julia-gen-jupyter\"></a>\n",
    "\n",
    "Gen is a package for the Julia language. The package can be loaded with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gen programs typically consist of a combination of (i) probabilistic models written in modeling languages and (ii) inference programs written in regular Julia code. Gen provides a built-in modeling language that is itself based on Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses a Jupyter notebook. All cells in the notebook are regular Julia cells. In Julia, semicolons are optional at the end of statements; we will use them at the end of some cells so that the value of the cell is not printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 1 + 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the [PyPlot](https://github.com/JuliaPy/PyPlot.jl) Julia package for plotting. PyPlot wraps the matplotlib Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using PyPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will make use of Julia symbols. Note that a Julia symbol is different from a Julia string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(:foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(\"foo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also load the `GenViz` Julia package, which is a JavaScript-based visualization framework designed for use with Gen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using GenViz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start a GenViz visualization server that we will use throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "viz_server = VizServer(8000)\n",
    "sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing a probabilistic model as a generative function  <a name=\"writing-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic models are represented in Gen as *generative functions*.\n",
    "The simplest way to construct a generative function is by using the [built-in modeling DSL](https://probcomp.github.io/Gen/dev/ref/modeling/). Generative functions written in the built-in modeling DSL are based on Julia function definition syntax, but are prefixed with the `@gen` keyword. The function represents the data-generating process we are modeling: each random choice it makes can be thought of as a random variable in the model.\n",
    "The generative function below represents a probabilistic model of a linear relationship in the x-y plane. Given a set of $x$ coordinates, it randomly chooses a line in the plane and generates corresponding $y$ coordinates so that each $(x, y)$ is near the line. We might think of this function as modeling house prices as a function of square footage, or the measured volume of a gas as a function of its measured temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function line_model(xs::Vector{Float64})\n",
    "    n = length(xs)\n",
    "    \n",
    "    # We begin by sampling a slope and intercept for the line.\n",
    "    # Before we have seen the data, we don't know the values of\n",
    "    # these parameters, so we treat them as random choices. The\n",
    "    # distributions they are drawn from represent our prior beliefs\n",
    "    # about the parameters: in this case, that neither the slope nor the\n",
    "    # intercept will be more than a couple points away from 0.\n",
    "    slope = @addr(normal(0, 1), :slope)\n",
    "    intercept = @addr(normal(0, 2), :intercept)\n",
    "    \n",
    "    # Given the slope and intercept, we can sample y coordinates\n",
    "    # for each of the x coordinates in our input vector.\n",
    "    for (i, x) in enumerate(xs)\n",
    "        @addr(normal(slope * x + intercept, 0.1), (:y, i))\n",
    "    end\n",
    "    \n",
    "    # The return value of the model is often not particularly important,\n",
    "    # Here, we simply return n, the number of points.\n",
    "    return n\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative function takes as an argument a vector of x-coordinates. We create one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = [-5., -4., -3., -.2, -1., 0., 1., 2., 3., 4., 5.];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this vector, the generative function samples a random choice representing the slope of a line from a normal distribution with mean 0 and standard deviation 1, and a random choice representing the intercept of a line from a normal distribution with mean 0 and standard deviation 2. In Bayesian statistics terms, these distributions are the *prior distributions* of the slope and intercept respectively. Then, the function samples values for the y-coordinates corresponding to each of the provided x-coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generative function returns the number of data points. We can run the function like we run a regular Julia function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = line_model(xs)\n",
    "println(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More interesting than `n` are the values of the random choies that `line_model` makes. **Crucially, each random choice is annotated with a unique *address*.** A random choice is assigned an address using the `@addr` keyword. Addresses can be any Julia value. In this program, there are two types of addresses used -- Julia symbols and tuples of symbols and integers. Note that within the `for` loop, the same line of code is executed multiple times, but each time, the random choice it makes is given a distinct address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the random choices are not included in the return value, they *are* included in the *execution trace* of the generative function. We can run the generative function and obtain its trace using the `\n",
    "initialize` method from the Gen API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trace, _) = Gen.initialize(line_model, (xs,));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method takes the function to be executed, and a tuple of arguments to the function, and returns a trace and a second value that we will not be using in this tutorial. When we print the trace, we see that it is a complex data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "println(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trace of a generative function contains various information about an execution of the function. For example, it contains the arguments on which the function was run, which are available with the API method `get_args`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Gen.get_args(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace also contains the value of the random choices, stored in map from address to value. This map is available through the API method `get_assmt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "println(Gen.get_assmt(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pull out individual values using Julia's subscripting syntax `[...]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "choices = Gen.get_assmt(trace)\n",
    "println(choices[:slope])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value is also recorded in the trace, and is accessible with the `get_retval` API method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "println(Gen.get_retval(trace));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the probabilistic behavior of a generative function, it is helpful to be able to visualize its traces. Below, we define a function that uses PyPlot to render a trace of the generative function above. The rendering shows the x-y data points and the line that is represented by the slope and intercept choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function render_trace(trace; show_data=true)\n",
    "    \n",
    "    # Pull out xs from the trace\n",
    "    xs = get_args(trace)[1]\n",
    "    \n",
    "    xmin = minimum(xs)\n",
    "    xmax = maximum(xs)\n",
    "    assmt = get_assmt(trace)\n",
    "    if show_data\n",
    "        ys = [assmt[(:y, i)] for i=1:length(xs)]\n",
    "        \n",
    "        # Plot the data set\n",
    "        scatter(xs, ys, c=\"black\")\n",
    "    end\n",
    "    \n",
    "    # Pull out slope and intercept from the trace\n",
    "    slope = assmt[:slope]\n",
    "    intercept = assmt[:intercept]\n",
    "    \n",
    "    # Draw the line\n",
    "    plot([xmin, xmax], slope *  [xmin, xmax] .+ intercept, color=\"black\", alpha=0.5)\n",
    "    ax = gca()\n",
    "    ax[:set_xlim]((xmin, xmax))\n",
    "    ax[:set_ylim]((xmin, xmax))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(3,3))\n",
    "render_trace(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because a generative function is stochastic, we need to visualize many runs in order to understand its behavior. The cell below renders a grid of traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function grid(renderer::Function, traces; ncols=6, nrows=3)\n",
    "    figure(figsize=(16, 8))\n",
    "    for (i, trace) in enumerate(traces)\n",
    "        subplot(nrows, ncols, i)\n",
    "        renderer(trace)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate several traces and render them in a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "traces = [initialize(line_model, (xs,))[1] for _=1:12]\n",
    "grid(render_trace, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------\n",
    "### Exercise\n",
    "List the addresses of all random choices made when applying `line_model` to the vector `xs` defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------\n",
    "### Exercise\n",
    "\n",
    "Write a generative function that uses the same address twice. Run it to see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "### Exercise\n",
    "\n",
    "Write a model that generates a sine wave with random phase, period and amplitude, and then generates y-coordinates from a given vector of x-coordinates by adding noise to the value of the wave at each x-coordinate.\n",
    "Use a  `gamma(5, 1)` prior distribution for the period, and a `gamma(1, 1)` prior distribution on the amplitude (see [`Gen.gamma`](https://probcomp.github.io/Gen/dev/ref/distributions/#Gen.gamma)). Use a uniform distribution for the phase (see [`Gen.uniform`](https://probcomp.github.io/Gen/dev/ref/distributions/#Gen.uniform)). Write a function that renders the trace by showing the data set and the sine wave. Visualize a grid of traces and discuss the distribution. Try tweaking the parameters of each of the prior distributions and seeing how the behavior changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided you with some starter code for the sine wave model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function sine_model(xs::Vector{Float64})\n",
    "    n = length(xs)\n",
    " \n",
    "    # < your code here >\n",
    " \n",
    "    for (i, x) in enumerate(xs)\n",
    "        @addr(normal(0., 0.1), (:y, i)) # < edit this line >\n",
    "    end\n",
    "    return n\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function render_sine_trace(trace; show_data=true)\n",
    "    xs = get_args(trace)[1]\n",
    "    xmin = minimum(xs)\n",
    "    xmax = maximum(xs)\n",
    "    assmt = get_assmt(trace)\n",
    "    if show_data\n",
    "        ys = [assmt[(:y, i)] for i=1:length(xs)]\n",
    "        scatter(xs, ys, c=\"black\")\n",
    "    end\n",
    "    \n",
    "    # < your code here >\n",
    "    \n",
    "    ax = gca()\n",
    "    ax[:set_xlim]((xmin, xmax))\n",
    "    ax[:set_ylim]((xmin, xmax))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traces = [initialize(sine_model, (xs,))[1] for _=1:12];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(16, 8))\n",
    "for (i, trace) in enumerate(traces)\n",
    "    subplot(3, 6, i)\n",
    "    render_sine_trace(trace)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Doing Posterior inference  <a name=\"doing-inference\"></a>\n",
    "\n",
    "We now will provide a data set of y-coordinates and try to draw inferences about the process that generated the data. We begin with the following data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys = [6.75003, 6.1568, 4.26414, 1.84894, 3.09686, 1.94026, 1.36411, -0.83959, -0.976, -1.93363, -2.91303];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(3,3))\n",
    "scatter(xs, ys, color=\"black\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that the line model was responsible for generating the data, and infer values of the slope and intercept that explain the data.\n",
    "\n",
    "To do this, we write a simple *inference program* that takes the model we are assuming generated our data, the data set, and the amount of computation to perform, and returns a trace of the function that is approximately sampled from the _posterior distribution_ on traces of the function, given the observed data. That is, the inference program will try to find a trace that well explains the dataset we created above. We can inspect that trace to find estimates of the slope and intercept of a line that fits the data.\n",
    "\n",
    "Functions like `importance_resampling` expect us to provide a _model_ and also an _assignment_ representing our data set and relating it to the model. An assignment maps random choice addresses from the mod\n",
    "el to values from our data set. Here, we want to tie model addresses like `(:y, 4)` to data set values like `ys[4]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function do_inference(model, xs, ys, amount_of_computation)\n",
    "    \n",
    "    # Create an \"Assignment\" that maps model addresses (:y, i)\n",
    "    # to observed values ys[i]. We leave :slope and :intercept\n",
    "    # unconstrained, because we want them to be inferred.\n",
    "    observations = Gen.DynamicAssignment()\n",
    "    for (i, y) in enumerate(ys)\n",
    "        observations[(:y, i)] = y\n",
    "    end\n",
    "    \n",
    "    # Call importance_resampling to obtain a likely trace consistent\n",
    "    # with our observations.\n",
    "    (trace, _) = Gen.importance_resampling(model, (xs,), observations, amount_of_computation);\n",
    "    return trace\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the inference program to obtain a trace, and then visualize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace = do_inference(line_model, xs, ys, 100)\n",
    "figure(figsize=(3,3))\n",
    "render_trace(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `importance_resampling` found a reasonable slope and intercept to explain the data. We can also visualize many samples in a grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traces = [do_inference(line_model, xs, ys, 100) for _=1:10];\n",
    "grid(render_trace, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that there is some uncertainty: with our limited data, we can't be 100% sure exactly where the line is. We can get a better sense for the variability in the posterior distribution by visualizing all the traces in one plot, rather than in a grid. Each trace is going to have the same observed data points, so we only plot those once, based on the values in the first trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function overlay(renderer, traces; same_data=true, args...)\n",
    "    renderer(traces[1], show_data=true, args...)\n",
    "    for i=2:length(traces)\n",
    "        renderer(traces[i], show_data=!same_data, args...)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traces = [do_inference(line_model, xs, ys, 100) for _=1:10];\n",
    "figure(figsize=(3,3))\n",
    "overlay(render_trace, traces);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "### Exercise\n",
    "\n",
    "The results above were obtained for `amount_of_computation = 100`. Run the algorithm with this value set to `1`, `10`, and `1000`, etc.  Which value seems like a good tradeoff between accuracy and running time? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "### Exercise\n",
    "Consider the following data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys_sine = [2.89, 2.22, -0.612, -0.522, -2.65, -0.133, 2.70, 2.77, 0.425, -2.11, -2.76];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(3, 3));\n",
    "scatter(xs, ys_sine, color=\"black\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write an inference program that generates traces of `sine_model` that explain this data set. Visualize the resulting distribution of traces. Temporarily change the prior distribution on the period to be `gamma(1, 1)`  (by changing and re-running the cell that defines `sine_model` from a previous exercise). Can you explain the difference in inference results when using `gamma(1, 1)` vs `gamma(5, 1)` prior on the period? How much computation did you need to get good results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predicting new data  <a name=\"predicting-data\"></a>\n",
    "\n",
    "By providing a third argument to `Gen.initialize`, it is possible to run a generative function with the values of certain random choices constrained to given values. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "constraints = Gen.DynamicAssignment()\n",
    "constraints[:slope] = 0.\n",
    "constraints[:intercept] = 0.\n",
    "(trace, _) = Gen.initialize(line_model, (xs,), constraints)\n",
    "figure(figsize=(3,3))\n",
    "render_trace(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the random choices corresponding to the y-coordinates are still made randomly. Run the cell above a few times to verify this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the ability to run constrained executions of a generative function to predict the value of the y-coordinates at new x-coordinates by running new executions of the model generative function in which the random choices corresponding to the parameters have been constrained to their inferred values.  We have provided a function below (`predict_new_data`) that takes a trace, and a vector of new x-coordinates, and returns a vector of predicted y-coordinates corresponding to the x-coordinates in `new_xs`. We have designed this function to work with multiple models, so the set of parameter addresses is an argument (`param_addrs`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function predict_new_data(model, trace, new_xs::Vector{Float64}, param_addrs)\n",
    "    \n",
    "    # Copy parameter values from the inferred trace (`trace`)\n",
    "    # into a fresh set of constraints.\n",
    "    assmt = Gen.get_assmt(trace)\n",
    "    constraints = Gen.DynamicAssignment()\n",
    "    for addr in param_addrs\n",
    "        constraints[addr] = assmt[addr]\n",
    "    end\n",
    "    \n",
    "    # Run the model with new x coordinates, and with parameters \n",
    "    # fixed to be the inferred values\n",
    "    (new_trace, _) = Gen.initialize(model, (new_xs,), constraints)\n",
    "    \n",
    "    # Pull out the y-values and return them\n",
    "    new_assmt = Gen.get_assmt(new_trace)\n",
    "    ys = [new_assmt[(:y, i)] for i=1:length(new_xs)]\n",
    "    return ys\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines a function that first performs inference on an observed data set `(xs, ys)`, and then runs `predict_new_data` to generate predicted y-coordinates. It repeats this process `num_traces` times, and returns a vector of the resulting y-coordinate vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function infer_and_predict(model, xs, ys, new_xs, param_addrs, num_traces, amount_of_computation)\n",
    "    pred_ys = []\n",
    "    for i=1:num_traces\n",
    "        trace = do_inference(model, xs, ys, amount_of_computation)\n",
    "        push!(pred_ys, predict_new_data(model, trace, new_xs, param_addrs))\n",
    "    end\n",
    "    pred_ys\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a cell that plots the observed data set `(xs, ys)` as red dots, and the predicted data as small black dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function plot_predictions(xs, ys, new_xs, pred_ys)\n",
    "    scatter(xs, ys, color=\"red\")\n",
    "    for pred_ys_single in pred_ys\n",
    "        scatter(new_xs, pred_ys_single, color=\"black\", s=1, alpha=0.3)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the original dataset for the line model. The x-coordinates span the interval -5 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(3,3))\n",
    "scatter(xs, ys, color=\"red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the inferred values of the parameters to predict y-coordinates for x-coordinates in the interval 5 to 10 from which data was not observed. We will also predict new data within the interval -5 to 5, and we will compare this data to the original observed data. Predicting new data from inferred parameters, and comparing this new data to the observed data is the core idea behind *posterior predictive checking*. This tutorial does not intend to give a rigorous overview behind techniques for checking the quality of a model, but intends to give high-level intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_xs = collect(range(-5, stop=10, length=100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate and plot the predicted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_ys = infer_and_predict(line_model, xs, ys, new_xs, [:slope, :intercept], 20, 1000)\n",
    "figure(figsize=(3,3))\n",
    "plot_predictions(xs, ys, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look reasonable, both within the interval of observed data and in the extrapolated predictions on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the same experiment run with following data set, which has significantly more noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys_noisy = [5.092, 4.781, 2.46815, 1.23047, 0.903318, 1.11819, 2.10808, 1.09198, 0.0203789, -2.05068, 2.66031];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_ys = infer_and_predict(line_model, xs, ys_noisy, new_xs, [:slope, :intercept], 20, 1000)\n",
    "figure(figsize=(3,3))\n",
    "plot_predictions(xs, ys_noisy, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the generated data is less noisy than the observed data in the regime where data was observed, and it looks like the forecasted data is too overconfident. This is a sign that our model is mis-specified. In our case, this is because we have assumed that the noise has value 0.1. However, the actual noise in the data appears to be much larger. We can correct this by making the noise a random choice as well and inferring its value along with the other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first write a new version of the line model that samples a random choice for the noise from a `gamma(1, 1)` prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function line_model_2(xs::Vector{Float64})\n",
    "    n = length(xs)\n",
    "    slope = @addr(normal(0, 1), :slope)\n",
    "    intercept = @addr(normal(0, 2), :intercept)\n",
    "    noise = @addr(gamma(1, 1), :noise)\n",
    "    for (i, x) in enumerate(xs)\n",
    "        @addr(normal(slope * x + intercept, noise), (:y, i))\n",
    "    end\n",
    "    return nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we compare the predictions using inference the unmodified and modified model on the `ys` data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(6,3))\n",
    "\n",
    "pred_ys = infer_and_predict(line_model, xs, ys, new_xs, [:slope, :intercept], 20, 1000)\n",
    "subplot(1, 2, 1)\n",
    "plot_predictions(xs, ys, new_xs, pred_ys)\n",
    "\n",
    "pred_ys = infer_and_predict(line_model_2, xs, ys, new_xs, [:slope, :intercept, :noise], 20, 10000)\n",
    "subplot(1, 2, 2)\n",
    "plot_predictions(xs, ys, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is more uncertainty in the predictions made using the modified model.\n",
    "\n",
    "We also compare the predictions using inference the unmodified and modified model on the `ys_noisy` data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(6,3))\n",
    "\n",
    "pred_ys = infer_and_predict(line_model, xs, ys_noisy, new_xs, [:slope, :intercept], 20, 1000)\n",
    "subplot(1, 2, 1)\n",
    "plot_predictions(xs, ys_noisy, new_xs, pred_ys)\n",
    "\n",
    "pred_ys = infer_and_predict(line_model_2, xs, ys_noisy, new_xs, [:slope, :intercept, :noise], 20, 10000)\n",
    "subplot(1, 2, 2)\n",
    "plot_predictions(xs, ys_noisy, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that while the unmodified model was very overconfident, the modified model has an appropriate level of uncertainty, while still capturing the general negative trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "### Exercise\n",
    "\n",
    "Write a modified version the sine model that makes noise into a random choice. Compare the predicted data with the observed data `infer_and_predict` and `plot_predictions` for the unmodified and modified model, and for the `ys_sine` and `ys_noisy` datasets. Discuss the results. Experiment with the amount of inference computation used. The amount of inference computation will need to be higher for the model with the noise random choice.\n",
    "\n",
    "We have provided you with starter code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function sine_model_2(xs::Vector{Float64})\n",
    "    n = length(xs)\n",
    "    \n",
    "    # < your code here >\n",
    "    \n",
    "    for (i, x) in enumerate(xs)\n",
    "        @addr(normal(0., 0.1), (:y, i)) # < edit this line >\n",
    "    end\n",
    "    return n\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(6,3))\n",
    "\n",
    "# Modify the line below>\n",
    "pred_ys = infer_and_predict(sine_model, xs, ys_sine, new_xs, [], 20, 1)\n",
    "\n",
    "subplot(1, 2, 1)\n",
    "plot_predictions(xs, ys_sine, new_xs, pred_ys)\n",
    "\n",
    "# Modify the line below>\n",
    "pred_ys = infer_and_predict(sine_model_2, xs, ys_sine, new_xs, [], 20, 1)\n",
    "\n",
    "subplot(1, 2, 2)\n",
    "plot_predictions(xs, ys_sine, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(6,3))\n",
    "\n",
    "# Modify the line below>\n",
    "pred_ys = infer_and_predict(sine_model, xs, ys_noisy, new_xs, [], 20, 1)\n",
    "\n",
    "subplot(1, 2, 1)\n",
    "plot_predictions(xs, ys_noisy, new_xs, pred_ys)\n",
    "\n",
    "# Modify the line below>\n",
    "pred_ys = infer_and_predict(sine_model_2, xs, ys_noisy, new_xs, [], 20, 1)\n",
    "\n",
    "subplot(1, 2, 2)\n",
    "plot_predictions(xs, ys_noisy, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calling other generative functions  <a name=\"calling-functions\"></a>\n",
    "\n",
    "In addition to making random choices, generative functions can invoke other generative functions. To illustrate this, we will write a probabilistic model that combines the line model and the sine model. This model is able to explain data using either model, and which model is chosen will depend on the data. This is called *model selection*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generative function can invoke another generative function in three ways:\n",
    "\n",
    "- using regular Julia function call syntax\n",
    "\n",
    "- using the `@splice` Gen keyword\n",
    "\n",
    "- using the `@addr` Gen keyword.\n",
    "\n",
    "When invoking using regular function call syntax, the random choices made by the callee function are not traced. When invoking using `@splice`, the random choices of the callee function are placed in the same address namespace as the caller's random choices. When using `@addr(<call>, <addr>)`, the random choices of the callee are placed under the namespace `<addr>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function foo()\n",
    "    @addr(normal(0, 1), :y)\n",
    "end\n",
    "\n",
    "@gen function bar_splice()\n",
    "    @addr(bernoulli(0.5), :x)\n",
    "    @splice(foo())\n",
    "end\n",
    "\n",
    "@gen function bar_addr()\n",
    "    @addr(bernoulli(0.5), :x)\n",
    "    @addr(foo(), :z)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first show the addresses sampled by `bar_splice`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trace, ) = initialize(bar_splice, ())\n",
    "println(get_assmt(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the addresses sampled by `bar_addr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trace, ) = initialize(bar_addr, ())\n",
    "println(get_assmt(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `@addr` instead of `@splice` can help avoid address collisions for complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we write a generative function that combies the line and sine models. It makes a Bernoulli random choice (e.g. a coin flip that returns true or false) that determines which of the two models will generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function combined_model(xs::Vector{Float64})\n",
    "    if @addr(bernoulli(0.5), :is_line)\n",
    "        @splice(line_model_2(xs))\n",
    "    else\n",
    "        @splice(sine_model_2(xs))\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also write a visualization for a trace of this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function render_combined(trace; show_data=true)\n",
    "    assmt = get_assmt(trace)\n",
    "    if assmt[:is_line]\n",
    "        render_trace(trace, show_data=show_data)\n",
    "    else\n",
    "        render_sine_trace(trace, show_data=show_data)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize some traces, and see that sometimes it samples linear data and other times sinusoidal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traces = [initialize(combined_model, (xs,))[1] for _=1:12];\n",
    "grid(render_combined, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run inference using this combined model on the `ys` data set and the `ys_sine` data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(6,3))\n",
    "subplot(1, 2, 1)\n",
    "traces = [do_inference(combined_model, xs, ys, 10000) for _=1:10];\n",
    "overlay(render_combined, traces)\n",
    "subplot(1, 2, 2)\n",
    "traces = [do_inference(combined_model, xs, ys_sine, 10000) for _=1:10];\n",
    "overlay(render_combined, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should show that the line model was inferred for the `ys` data set, and the sine wave model was inferred for the `ys_sine` data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Construct a data set for which it is ambiguous whether the line or sine wave model is best. Visualize the inferred traces using `render_combined` to illustrate the ambiguity. Write a program that takes the data set and returns an estimate of the posterior probability that the data was generated by the sine wave model, and run it on your data set.\n",
    "\n",
    "Hint: To estimate the posterior probability that the data was generated by the sine wave model, run the inference program many times to compute a large number of traces, and then compute the fraction of those traces in which `:is_line` is false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Exercise \n",
    "\n",
    "There is code that is duplicated between `line_model_2` and `sine_model_2`. Refactor the model to reduce code duplication and improve the readability of the code. Re-run the experiment above and confirm that the results are qualitatively the same. You may need to write a new rendering function. Try to avoid introducing code duplication between the model and the rendering code.\n",
    "\n",
    "Hint: To avoid introducing code duplication between the model and the rendering code, use the return value of the generative function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function line_model_refactored()\n",
    "    # < your code here >\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function sine_model_refactored()\n",
    "    # < your code here >\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function combined_model_refactored(xs::Vector{Float64})\n",
    "    # < your code here >\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function render_combined_refactored(trace; show_data=true)\n",
    "    xs = get_args(trace)[1]\n",
    "    xmin = minimum(xs)\n",
    "    xmax = maximum(xs)\n",
    "    assmt = get_assmt(trace)\n",
    "    if show_data\n",
    "        ys = [assmt[(:y, i)] for i=1:length(xs)]\n",
    "        scatter(xs, ys, c=\"black\")\n",
    "    end\n",
    "\n",
    "    # < your code here >\n",
    "    \n",
    "    ax = gca()\n",
    "    ax[:set_xlim]((xmin, xmax))\n",
    "    ax[:set_ylim]((xmin, xmax))\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(6,3))\n",
    "subplot(1, 2, 1)\n",
    "traces = [do_inference(combined_model_refactored, xs, ys, 10000) for _=1:10];\n",
    "overlay(render_combined_refactored, traces)\n",
    "subplot(1, 2, 2)\n",
    "traces = [do_inference(combined_model_refactored, xs, ys_sine, 10000) for _=1:10];\n",
    "overlay(render_combined_refactored, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling with an unbounded number of parameters  <a name=\"infinite-space\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gen's built-in modeling language can be used to express models that use an unbounded number of parameters. This section walks you through development of a model of data that does not a-priori specify an upper bound on the complexity model, but instead infers the complexity of the model as well as the parameters. This is a simple example of a *Bayesian nonparametric* model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider two data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs_dense = collect(range(-5, stop=5, length=50))\n",
    "ys_simple = fill(1., length(xs_dense)) .+ randn(length(xs_dense)) * 0.1\n",
    "ys_complex = [Int(floor(abs(x/3))) % 2 == 0 ? 2 : 0 for x in xs_dense] .+ randn(length(xs_dense)) * 0.1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(6,3))\n",
    "subplot(1, 2, 1)\n",
    "scatter(xs_dense, ys_simple, color=\"black\", s=10)\n",
    "gca()[:set_ylim]((-1, 3))\n",
    "subplot(1, 2, 2)\n",
    "scatter(xs_dense, ys_complex, color=\"black\", s=10);\n",
    "gca()[:set_ylim]((-1, 3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set on the left appears to be best explained as a contant function with some noise. The data set on the right appears to include four changepoints, with a constant function in between the changepoints. We want a model that does not a-priori choose the number of changepoints in the data. To do this, we will recursively partition the interval into regions. We define a Julia data structure that represents a binary tree of intervals; each leaf node represents a region in which the function is constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "struct Interval\n",
    "    l::Float64\n",
    "    u::Float64\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract type Node end\n",
    "    \n",
    "struct InternalNode <: Node\n",
    "    left::Node\n",
    "    right::Node\n",
    "    interval::Interval\n",
    "end\n",
    "\n",
    "struct LeafNode <: Node\n",
    "    value::Float64\n",
    "    interval::Interval\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a generative function that randomly creates such a tree. Note the use of recursion in this function to create arbitrarily large trees representing arbitrarily many changepoints. Also note the use of `@addr` instead of `@splice` for the recursive calls: this gives each new call its own address namespace, ensuring that each random choice made by `generate_segments` will have a distinct address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function generate_segments(l::Float64, u::Float64)\n",
    "    interval = Interval(l, u)\n",
    "    if @addr(bernoulli(0.7), :isleaf)\n",
    "        value = @addr(normal(0, 1), :value)\n",
    "        return LeafNode(value, interval)\n",
    "    else\n",
    "        frac = @addr(beta(2, 2), :frac)\n",
    "        mid  = l + (u - l) * frac\n",
    "        left = @addr(generate_segments(l, mid), :left)\n",
    "        right = @addr(generate_segments(mid, u), :right)\n",
    "        return InternalNode(left, right, interval)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define some helper functions to visualize traces of the `generate_segments` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function render_node(node::LeafNode)\n",
    "    plot([node.interval.l, node.interval.u], [node.value, node.value])\n",
    "end\n",
    "\n",
    "function render_node(node::InternalNode)\n",
    "    render_node(node.left)\n",
    "    render_node(node.right)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function render_segments_trace(trace)\n",
    "    node = get_retval(trace)\n",
    "    render_node(node)\n",
    "    ax = gca()\n",
    "    ax[:set_xlim]((0, 1))\n",
    "    ax[:set_ylim]((-3, 3))\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate 12 traces from this function and visualize them below. We plot the piecewise constant function that was sampled by each run of the generative function. Different constant segments are shown in different colors. Run the cell a few times to get a better sense of the distribution on functions that is represented by the generative function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traces = [initialize(generate_segments, (0., 1.))[1] for i=1:12]\n",
    "grid(render_segments_trace, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we only sub-divide an interval with 30% probability, most of these sampled traces have only one segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generative function that generates a random piecewise-constant function, we write a model that adds noise to the resulting constant functions to generate a data set of y-coordinates. The noise level will be a random choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get_value_at searches a binary tree for\n",
    "# the leaf node containing some value.\n",
    "function get_value_at(x::Float64, node::LeafNode)\n",
    "    @assert x >= node.interval.l && x <= node.interval.u\n",
    "    return node.value\n",
    "end\n",
    "\n",
    "function get_value_at(x::Float64, node::InternalNode)\n",
    "    @assert x >= node.interval.l && x <= node.interval.u\n",
    "    if x <= node.left.interval.u\n",
    "        get_value_at(x, node.left)\n",
    "    else\n",
    "        get_value_at(x, node.right)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Out full model\n",
    "@gen function changepoint_model(xs::Vector{Float64})\n",
    "    node = @addr(generate_segments(minimum(xs), maximum(xs)), :tree)\n",
    "    noise = @addr(gamma(1, 1), :noise)\n",
    "    for (i, x) in enumerate(xs)\n",
    "        @addr(normal(get_value_at(x, node), noise), (:y, i))\n",
    "    end\n",
    "    return node\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a visualization for `changepoint_model` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function render_changepoint_model_trace(trace; show_data=true)\n",
    "    xs = get_args(trace)[1]\n",
    "    node = get_retval(trace)\n",
    "    render_node(node)\n",
    "    assmt = get_assmt(trace)\n",
    "    if show_data\n",
    "        ys = [assmt[(:y, i)] for i=1:length(xs)]\n",
    "        scatter(xs, ys, c=\"black\")\n",
    "    end\n",
    "    ax = gca()\n",
    "    ax[:set_xlim]((minimum(xs), maximum(xs)))\n",
    "    ax[:set_ylim]((-3, 3))\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate some simulated data sets and visualize them on top of the underlying piecewise constant function from which they were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traces = [initialize(changepoint_model, (xs_dense,))[1] for i=1:12]\n",
    "grid(render_changepoint_model_trace, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the amount of variability around the piecewise constant mean function differs from trace to trace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform inference for the simple data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traces = [do_inference(changepoint_model, xs_dense, ys_simple, 10000) for _=1:12];\n",
    "grid(render_changepoint_model_trace, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we inferred that the mean function that explains the data is a constant with very high probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inference about the complex data set, we use more computation. You can experiment with different amounts of computation to see how the quality of the inferences degrade with less computation. Note that we are using a very simple generic inference algorithm in this tutorial, which really isn't suited for this more complex task. In later tutorials, we will learn how to write more efficient algorithms, so that accurate results can be obtained with significantly less amount of computation. We will also see ways of annotating the model for better performance, no matter the inference algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "traces = [do_inference(changepoint_model, xs_dense, ys_complex, 100000) for _=1:12];\n",
    "grid(render_changepoint_model_trace, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that more segments are inferred for the more complex data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Exercise\n",
    "Write a function that takes a data set of x- and y-coordinates and plots the histogram of the probability distribution on the number of changepoints.\n",
    "Show the results for the `ys_simple` and `ys_complex` data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### Exercise\n",
    "Write a new version of `changepoint_model` that uses `@splice` to make the recursive calls instead of `@addr`.\n",
    "\n",
    "Hint: How can you label each node in a binary tree using an integer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modeling with black-box Julia code   <a name=\"black-box\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section shows that \"black-box\" code like algorithms and simulators can be included in probabilistic models that are expressed as generative functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write a generative probabilistic model of the motion of an intelligent agent that is navigating a two-dimensional scene. The model will be *algorithmic* --- it will invoke a path planning algorithm implemented in regular Julia code to generate the agent's motion plan from its destination its map of the scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load some basic geometric primitives for a two-dimensional scene in the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "include(\"../inverse-planning/geometric_primitives.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file defines two-dimensional `Point` data type with fields `x` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = Point(1.0, 2.0)\n",
    "println(point.x)\n",
    "println(point.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file also defines an `Obstacle` data type, which represents a polygonal obstacle in a two-dimensional scene, that is constructed from a list of vertices. Here, we construct a square:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obstacle = Obstacle([Point(0.0, 0.0), Point(1.0, 0.0), Point(0.0, 1.0), Point(1.0, 1.0)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the definition of a `Scene` data type that represents a two-dimensional scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../inverse-planning/scene.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". The scene spans a rectangle of on the two-dimensional x-y plane, and contains a list of obstacles, which is initially empty:\n",
    "\n",
    "`scene = Scene(xmin::Real, xmax::Real, ymin::Real, ymax::real)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = Scene(0, 1, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obstacles are added to the scene with the `add_obstacle!` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_obstacle!(scene, obstacle);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file also defines functions that simplify the construction of obstacles:\n",
    "\n",
    "`make_square(center::Point, size::Float64)` constructs a square-shaped obstacle centered at the given point with the given side length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obstacle = make_square(Point(0.30, 0.20), 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_line(vertical::Bool, start::Point, length::Float64, thickness::Float64)` constructs an axis-aligned line (either vertical or horizontal) with given thickness that extends from a given strating point for a certain length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obstacle = make_line(false, Point(0.20, 0.40), 0.40, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now construct a scene value that we will use in the rest of the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = Scene(0, 1, 0, 1)\n",
    "add_obstacle!(scene, make_square(Point(0.30, 0.20), 0.1))\n",
    "add_obstacle!(scene, make_square(Point(0.83, 0.80), 0.1))\n",
    "add_obstacle!(scene, make_square(Point(0.80, 0.40), 0.1))\n",
    "horizontal = false\n",
    "vertical = true\n",
    "wall_thickness = 0.02\n",
    "add_obstacle!(scene, make_line(horizontal, Point(0.20, 0.40), 0.40, wall_thickness))\n",
    "add_obstacle!(scene, make_line(vertical, Point(0.60, 0.40), 0.40, wall_thickness))\n",
    "add_obstacle!(scene, make_line(horizontal, Point(0.60 - 0.15, 0.80), 0.15 + wall_thickness, wall_thickness))\n",
    "add_obstacle!(scene, make_line(horizontal, Point(0.20, 0.80), 0.15, wall_thickness))\n",
    "add_obstacle!(scene, make_line(vertical, Point(0.20, 0.40), 0.40, wall_thickness));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the scene below. Don't worry about the details of this cell. It should display a two-dimensional map of the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = Dict(\"scene\" => scene)\n",
    "viz = Viz(viz_server, joinpath(@__DIR__, \"../inverse-planning/overlay-viz/dist\"), info)\n",
    "displayInNotebook(viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load a file that defines a `Path` data type (a sequence of `Points`), and a `plan_path` method, which  uses a path planning algorithm based on rapidly exploring random tree (RRT, [1]) to find a sequence of `Point`s beginning with `start` and ending in `dest` such that the line segment between each consecutive pair of points does nt intersect any obstacles in the scene. The planning algorithm may fail to find a valid path, in which case it will return a value of type `Nothing`.\n",
    "\n",
    "`path::Union{Path,Nothing} = plan_path(start::Point, dest::Point, scene::Scene, planner_params::PlannerParams)`\n",
    "\n",
    "[1] Rapidly-exploring random trees: A new tool for path planning. S. M. LaValle. TR 98-11, Computer Science Dept., Iowa State University, October 1998,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../inverse-planning/planning.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `plan_path` to plan a path from the lower-left corner of the scene into the interior of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = Point(0.1, 0.1)\n",
    "dest = Point(0.5, 0.5)\n",
    "planner_params = PlannerParams(300, 3.0, 2000, 1.)\n",
    "path = plan_path(start, dest, scene, planner_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the path below. The start location is shown in blue, the destination in red, and the path in orange. Run the cell above followed by the cell below a few times to see the variability in the paths generated by `plan_path` for these inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = Dict(\"start\"=> start, \"dest\" => dest, \"scene\" => scene, \"path_edges\" => get_edges(path))\n",
    "viz = Viz(viz_server, joinpath(@__DIR__, \"../inverse-planning/overlay-viz/dist\"), info)\n",
    "displayInNotebook(viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a model for how the agent moves along its path.\n",
    "We will assume that the agent moves along its path a constant speed. The file loaded above also defines a method (`walk_path`) that computes the locations of the agent at a set of timepoints (sampled at time intervals of `dt` starting at time `0.`), given the path and the speed of the agent:\n",
    "\n",
    "`locations::Vector{Point} =  walk_path(path::Path, speed::Float64, dt::Float64, num_ticks::Int)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed = 1.\n",
    "dt = 0.1\n",
    "num_ticks = 10;\n",
    "locations = walk_path(path, speed, dt, num_ticks)\n",
    "println(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are prepated to write our generative model for the motion of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function agent_model(scene::Scene, dt::Float64, num_ticks::Int, planner_params::PlannerParams)\n",
    "\n",
    "    # sample the start point of the agent from the prior\n",
    "    start_x = @addr(uniform(0, 1), :start_x)\n",
    "    start_y = @addr(uniform(0, 1), :start_y)\n",
    "    start = Point(start_x, start_y)\n",
    "\n",
    "    # sample the destination point of the agent from the prior\n",
    "    dest_x = @addr(uniform(0, 1), :dest_x)\n",
    "    dest_y = @addr(uniform(0, 1), :dest_y)\n",
    "    dest = Point(dest_x, dest_y)\n",
    "\n",
    "    # plan a path that avoids obstacles in the scene\n",
    "    maybe_path = plan_path(start, dest, scene, planner_params)\n",
    "    planning_failed = maybe_path == nothing\n",
    "    \n",
    "    # sample the speed from the prior\n",
    "    speed = @addr(uniform(0, 1), :speed)\n",
    "\n",
    "    if planning_failed\n",
    "        \n",
    "        # path planning failed, assume the agent stays as the start location indefinitely\n",
    "        locations = fill(start, num_ticks)\n",
    "    else\n",
    "        \n",
    "        # path planning succeeded, move along the path at constant speed\n",
    "        locations = walk_path(maybe_path, speed, dt, num_ticks)\n",
    "    end\n",
    "\n",
    "    # generate noisy measurements of the agent's location at each time point\n",
    "    noise = 0.01\n",
    "    for (i, point) in enumerate(locations)\n",
    "        x = @addr(normal(point.x, noise), :meas => (i, :x))\n",
    "        y = @addr(normal(point.y, noise), :meas => (i, :y))\n",
    "    end\n",
    "\n",
    "    return (planning_failed, maybe_path)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform a traced execution of `agent_model` and print out the random choices it made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = Point(0.1, 0.1)\n",
    "dest = Point(0.5, 0.5)\n",
    "planner_params = PlannerParams(300, 3.0, 2000, 1.)\n",
    "(trace, _) = initialize(agent_model, (scene, dt, num_ticks, planner_params));\n",
    "assmt = get_assmt(trace)\n",
    "println(assmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we explore the assumptions of the model by sampling many traces from the generative function and visualizing them. We have created a visualization specialized for this generative function for use with the `GenViz` package, in the directory `../inverse-planning/grid-viz/dist`. We have also defined a `trace_to_dict` method to convert the trace into a value that can be automatically serialized into a JSON string for use by the visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function trace_to_dict(trace)\n",
    "    args = Gen.get_args(trace)\n",
    "    (scene, dt, num_ticks, planner_params) = args\n",
    "    choices = Gen.get_assmt(trace)\n",
    "    (planning_failed, maybe_path) = Gen.get_retval(trace)\n",
    "\n",
    "    d = Dict()\n",
    "\n",
    "    # scene (the obstacles)\n",
    "    d[\"scene\"] = scene\n",
    "\n",
    "    # the points along the planned path\n",
    "    if planning_failed\n",
    "        d[\"path\"] = []\n",
    "    else\n",
    "        d[\"path\"] = maybe_path.points\n",
    "    end\n",
    "\n",
    "    # start and destination location\n",
    "    d[\"start\"] = Point(choices[:start_x], choices[:start_y])\n",
    "    d[\"dest\"] = Point(choices[:dest_x], choices[:dest_y])\n",
    "\n",
    "    # the observed location of the agent over time\n",
    "    measurements = Vector{Point}(undef, num_ticks)\n",
    "    for i=1:num_ticks\n",
    "        measurements[i] = Point(choices[:meas => (i, :x)], choices[:meas => (i, :y)])\n",
    "    end\n",
    "    d[\"measurements\"] = measurements\n",
    "\n",
    "    return d\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some traces of the function, with the start location fixed to a point in the lower-left corner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "constraints = DynamicAssignment()\n",
    "constraints[:start_x] = 0.1\n",
    "constraints[:start_y] = 0.1\n",
    "\n",
    "viz = Viz(viz_server, joinpath(@__DIR__, \"../inverse-planning/grid-viz/dist\"), [])\n",
    "for i=1:12\n",
    "    (trace, _) = initialize(agent_model, (scene, dt, num_ticks, planner_params), constraints)\n",
    "    putTrace!(viz, i, trace_to_dict(trace))\n",
    "end\n",
    "displayInNotebook(viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this visualization, the start location is represented by a blue dot, and the destination is represented by a red dot. The measured coordinates at each time point are represented by black dots. The path, if path planning was succesfull, is shown as a gray line fro the start point to the destination point. Notice that the speed of the agent is different in each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Constrain the destination point to a location in the scene, and visualize the distribution on paths. Find a destination point where the distributions on paths is multimodal (i.e. there are two spatially separated paths that the agent may take). Describe the variability in the planned paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a simple algorithm for inferring the destination of an agent given (i) the scene, (ii) the start location of the agent, and (iii) a sequence of measured locations of the agent for each tick.\n",
    "\n",
    "We will assume the agent starts in the lower left-hand corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = Point(0.1, 0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will infer the destination of the agent for the given sequence of observed locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measurements = [\n",
    "    Point(0.0980245, 0.104775),\n",
    "    Point(0.113734, 0.150773),\n",
    "    Point(0.100412, 0.195499),\n",
    "    Point(0.114794, 0.237386),\n",
    "    Point(0.0957668, 0.277711),\n",
    "    Point(0.140181, 0.31304),\n",
    "    Point(0.124384, 0.356242),\n",
    "    Point(0.122272, 0.414463),\n",
    "    Point(0.124597, 0.462056),\n",
    "    Point(0.126227, 0.498338)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize this data set on top of the scene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "info = Dict(\"start\" => start, \"scene\" => scene, \"measurements\" => measurements)\n",
    "viz = Viz(viz_server, joinpath(@__DIR__, \"../inverse-planning/overlay-viz/dist\"), info)\n",
    "displayInNotebook(viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a simple inference program for this task. Note that this inference program is similar to our `do_inference` inference program used previously in this tutorial. The main difference is that the addresses of the observed random choices are different for this model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function do_inference_agent_model(scene::Scene, dt::Float64, num_ticks::Int, planner_params::PlannerParams, start::Point,\n",
    "                                  measurements::Vector{Point}, amount_of_computation::Int)\n",
    "    \n",
    "    # Create an \"Assignment\" that maps model addresses (:y, i)\n",
    "    # to observed values ys[i]. We leave :slope and :intercept\n",
    "    # unconstrained, because we want them to be inferred.\n",
    "    observations = Gen.DynamicAssignment()\n",
    "    observations[:start_x] = start.x\n",
    "    observations[:start_y] = start.y\n",
    "    for (i, m) in enumerate(measurements)\n",
    "        observations[:meas => (i, :x)] = m.x\n",
    "        observations[:meas => (i, :y)] = m.y\n",
    "    end\n",
    "    \n",
    "    # Call importance_resampling to obtain a likely trace consistent\n",
    "    # with our observations.\n",
    "    (trace, _) = Gen.importance_resampling(agent_model, (scene, dt, num_ticks, planner_params), observations, amount_of_computation)\n",
    "    \n",
    "    return trace\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we run this algorithm 1000 times, to generate 1000 approximate samples from the posterior distribution on the destination. The inferred destinations should appear as red dots on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = Dict(\"measurements\" => measurements, \"scene\" => scene, \"start\" => start)\n",
    "viz = Viz(viz_server, joinpath(@__DIR__, \"../inverse-planning/overlay-viz/dist\"), info)\n",
    "openInNotebook(viz)\n",
    "sleep(5)\n",
    "for i=1:1000\n",
    "    trace = do_inference_agent_model(scene, dt, num_ticks, planner_params, start, measurements, 50)\n",
    "    putTrace!(viz, i, trace_to_dict(trace))\n",
    "end\n",
    "displayInNotebook(viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "The first argument to `PlannerParams` is the number of iterations of the RRT algorithm to use. The third argument to `PlannerParams` is the number of iterations of path refinement. These parameters affect the distribution on paths of the agent. Visualize traces of the `agent_model` for with a couple different settings of these two parameters to the path planning algorithm for fixed starting point and destination point. Try setting them to smaller values. Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We have provide starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = DynamicAssignment()\n",
    "constraints[:start_x] = 0.1\n",
    "constraints[:start_y] = 0.1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the `PlannerParams` in the two cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_params = PlannerParams(300, 3.0, 2000, 1.) # < change this line>\n",
    "\n",
    "viz = Viz(viz_server, joinpath(@__DIR__, \"../inverse-planning/grid-viz/dist\"), [])\n",
    "for i=1:12\n",
    "    (trace, _) = initialize(agent_model, (scene, dt, num_ticks, planner_params), constraints)\n",
    "    putTrace!(viz, i, trace_to_dict(trace))\n",
    "end\n",
    "displayInNotebook(viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_params = PlannerParams(300, 3.0, 2000, 1.) # < change this line>\n",
    "\n",
    "viz = Viz(viz_server, joinpath(@__DIR__, \"../inverse-planning/grid-viz/dist\"), [])\n",
    "for i=1:12\n",
    "    (trace, _) = initialize(agent_model, (scene, dt, num_ticks, planner_params), constraints)\n",
    "    putTrace!(viz, i, trace_to_dict(trace))\n",
    "end\n",
    "displayInNotebook(viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8. Modeling with TensorFlow code <a name=\"tensorflow\"></a>\n",
    "\n",
    "So far, we have seen generative functions that are defined only using the built-in modeling language, which uses the `@gen` keyword. However, Gen can also be extended with other modeling languages, as long as they produce generative functions that implement the [Generative Function Interface](https://probcomp.github.io/Gen/dev/ref/gfi/). The [GenTF](https://github.com/probcomp/GenTF) Julia package provides one such modeling language which allow generative functions to be constructed from user-defined TensorFlow computation graphs. Generative functions written in the built-in language can invoke generative functions defined using the GenTF language.\n",
    "\n",
    "This section shows how to write a generative function in the GenTF language, how to invoke a GenTF generative function from a `@gen` function, and how to perform basic supervised training of a generative function. Specifically, we will train a softmax regression conditional inference model to generate the label of an MNIST digit given the pixels.\n",
    "\n",
    "Later tutorials will show how to use deep learning and TensorFlow to accelerate inference in generative models, using ideas from \"amortized inference\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Only attempt to run this section if you have a working installation of TensorFlow and GenTF (see the [GenTF installation instructions](https://probcomp.github.io/GenTF/dev/#Installation-1))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the GenTF package and the PyCall package. The PyCall package is used because TensorFlow computation graphs are constructed using the TensorFlow Python API, and the PyCall package allows Python code to be run from Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using GenTF\n",
    "using PyCall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We text load the TensorFlow and TensorFlow.nn Python modules into our scope. The `@pyimport` macro is defined by PyCall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@pyimport tensorflow as tf\n",
    "@pyimport tensorflow.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a TensorFlow computation graph. The graph will have placeholders for an N x 784 matrix of pixel values, where N is the number of images that will be processed in batch, and 784 is the number of pixels in an MNIST image (28x28). There are 10 possible digit classes. The `probs` Tensor is an N x 10 matrix, where each row of the matrix is the vector of normalized probabilities of each digit class for a single input image. Note that this code is largely identical to the corresponding Python code. We provide initial values for the weight and bias parameters that are computed in Julia (it is also possible to use TensorFlow initializers for this purpose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input images, shape (N, 784)\n",
    "xs = tf.placeholder(tf.float64)\n",
    "\n",
    "# weight matrix parameter for soft-max regression, shape (784, 10)\n",
    "# initialize to a zeros matrix generated by Julia.\n",
    "init_W = zeros(Float64, 784, 10)\n",
    "W = tf.Variable(init_W)\n",
    "    \n",
    "# bias vector parameter for soft-max regression, shape (10,)\n",
    "# initialize to a zeros vector generated by Julia.\n",
    "init_b = zeros(Float64, 10)\n",
    "b = tf.Variable(init_b)\n",
    "\n",
    "# probabilities for each class, shape (N, 10)\n",
    "probs = nn.softmax(tf.add(tf.matmul(xs, W), b), axis=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we construct the generative function from this graph. The GenTF package provides a `TFFunction` type that implements the generative function interface. The `TFFunction` constructor takes:\n",
    "\n",
    "(i) A vector of Tensor objects that will be the trainable parameters of the generative function (`[W, b]`). These should be TensorFlow variables.\n",
    "\n",
    "(ii) A vector of Tensor object that are the inputs to the generative function (`[xs]`). These should be TensorFlow placeholders.\n",
    "\n",
    "(iii) The Tensor object that is the return value of the generative function (`probs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_softmax_model = TFFunction([W, b], [xs], probs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TFFunction` constructor creates a new TensorFlow session that will be used to execute all TensorFlow code for this generative function. It is also TensorFlow possible to supply a session explicitly to the constructor. See the [GenTF documentation](https://probcomp.github.io/GenTF/dev/) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the resulting generative function on some fake input data. This causes the TensorFlow to execute code in the TensorFlow session associated with `tf_softmax_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_xs = rand(5, 784)\n",
    "probs = tf_softmax_model(fake_xs)\n",
    "println(size(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `Gen.initialize` to obtain a trace of this generative function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trace, _) = Gen.initialize(tf_softmax_model, (fake_xs,));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that generative functions constructed using GenTF do not make random choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(Gen.get_assmt(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value is the Julia value corresponding to the Tensor `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(size(Gen.get_retval(trace)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write a generative function using the built-in modeling DSL that invokes the TFFunction generative function we just defined. Note that we wrap the call to `tf_softmax_model` in an `@addr` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function digit_model(xs::Matrix{Float64})\n",
    "    \n",
    "    # there are N input images, each with D pixels\n",
    "    (N, D) = size(xs)\n",
    "    \n",
    "    # invoke the `net` generative function to compute the digit label probabilities for all input images\n",
    "    probs = @addr(tf_softmax_model(xs), :softmax)\n",
    "    @assert size(probs) == (N, 10)\n",
    "    \n",
    "    # sample a digit label for each of the N input images\n",
    "    for i=1:N\n",
    "        @addr(categorical(probs[i,:]), (:y, i)) \n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's obtain a trace of `digit_model` on the fake tiny input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trace, _) = Gen.initialize(digit_model, (fake_xs,));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `net` generative function does not make any random choices. The only random choices are the digit labels for each input input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(Gen.get_assmt(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the `digit_model` will be useful for anything, it needs to be trained. We load some code for loading batches of MNIST training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "include(\"mnist.jl\")\n",
    "training_data_loader = MNISTTrainDataLoader();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the trainable parameters of the `tf_softmax_model` generative function  (`W` and `b`) on the MNIST traing data. Note that these parameters are stored as the state of the TensorFlow variables. We will use the [`Gen.train!`](https://probcomp.github.io/Gen/dev/ref/inference/#Gen.train!) method, which supports supervised training of generative functions using stochastic gradient opimization methods. In particular, this method takes the generative function to be trained (`digit_model`), a Julia function of no arguments that generates a batch of training data, and the update to apply to the trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ParamUpdate` constructor takes the type of update to perform (in this case a gradient descent update with step size 0.00001), and a specification of which trainable parameters should be updated). Here, we request that the `W` and `b` trainable parameters of the `tf_softmax_model` generative function should be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update = Gen.ParamUpdate(Gen.GradientDescent(0.00001, 1000000), tf_softmax_model => [W, b]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data generator, we obtain a batch of 100 MNIST training images. The data generator must return a tuple, where the first element is a set of arguments to the generative function being trained (`(xs,)`) and the second element contains the values of random choices. `train!` attempts to maximize the expected log probability of these random choices given their corresponding input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "function data_generator()\n",
    "    (xs, ys) = next_batch(training_data_loader, 100)\n",
    "\n",
    "    @assert size(xs) == (100, 784)\n",
    "    @assert size(ys) == (100,)\n",
    "    constraints = Gen.DynamicAssignment()\n",
    "    for (i, y) in enumerate(ys)\n",
    "        constraints[(:y, i)] = y\n",
    "    end\n",
    "    ((xs,), constraints)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run 10000 iterations of stochastic gradient descent, where each iteration uses a batch of 100 images to get a noisy gradient estimate. This might take one or two minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time scores = Gen.train!(digit_model, data_generator, update, 10000, 1, 1, 1; verbose=false);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot an estimate of the objective function function over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(scores)\n",
    "xlabel(\"iterations of stochastic gradient descent\")\n",
    "ylabel(\"Estimate of expected conditional log likelihood\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "It is common to \"vectorize\" deep learning code so that it runs on multiple inputs at a time. This is important for making efficient use of GPU resources when training. The TensorFlow code above is vectorized across images. Construct a new TFFunction that only runs on one image at a time, and use a Julia for loop over multiple invocations of this new TFFunction, one for each image. Run the training procedure for 100 iterations. Comment on the performance difference.\n",
    "\n",
    "NOTE: Even if you are not using a GPU, there should still be a noticeable performance difference, due to the overhead of invoking the Python / TensorFlow runtime stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We have provided you with starter code, including a new TensorFlow computation graph where `x` is a placeholder for a single image, and where `probs_unvec` are the digit class probabilities for a single image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input images, shape (784,)\n",
    "x = tf.placeholder(tf.float64)\n",
    "\n",
    "# weight matrix parameter for soft-max regression, shape (784, 10)\n",
    "# initialize to a zeros matrix generated by Julia.\n",
    "W_unvec = tf.Variable(init_W)\n",
    "    \n",
    "# bias vector parameter for soft-max regression, shape (10,)\n",
    "# initialize to a zeros vector generated by Julia.\n",
    "b_unvec = tf.Variable(init_b)\n",
    "\n",
    "# probabilities for each class, shape (10,)\n",
    "probs_unvec = tf.squeeze(nn.softmax(tf.add(tf.matmul(tf.expand_dims(x, axis=0), W_unvec), b_unvec), axis=1), axis=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide you with the definition of the new TFFunction based on this computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_softmax_model_single = TFFunction([W_unvec, b_unvec], [x], probs_unvec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an update that modifies the parameters of `tf_softmax_model_single`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update_unvec = ParamUpdate(GradientDescent(0.00001, 1000000), tf_softmax_model_single => [W_unvec, b_unvec]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the missing sections in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function digit_model_single_image(x::Vector{Float64})\n",
    "    \n",
    "    # number of pixels in the image\n",
    "    D = length(x)\n",
    "    \n",
    "    # < your code here >\n",
    "    \n",
    "    # sample the digit label for the image\n",
    "    @addr(categorical(probs), :y)\n",
    "    \n",
    "    return nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function digit_model_unvectorized(xs::Matrix{Float64})\n",
    "    (N, D) = size(xs)\n",
    "    for i=1:N\n",
    "\n",
    "         # < your code here >\n",
    "    \n",
    "    end\n",
    "    \n",
    "    return nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have filled in the cells above, try running `digit_model_unvectorized` on some input to help debug your solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, fill in the data generator that you will use to train the new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function data_generator_unvectorized()\n",
    "    (xs, ys) = next_batch(training_data_loader, 100)\n",
    "    @assert size(xs) == (100, 784)\n",
    "    @assert size(ys) == (100,)\n",
    "    \n",
    "    # < your code here >\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, perform the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@time scores_unvec = train!(digit_model_unvectorized, data_generator_unvectorized, update_unvec, 100, 1, 1, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(scores_unvec)\n",
    "xlabel(\"iterations of stochastic gradient descent\")\n",
    "ylabel(\"Estimate of expected conditional log likelihood\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
