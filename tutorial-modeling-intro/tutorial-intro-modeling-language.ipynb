{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Introduction to modeling in Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial introduces Gen's built-in modeling language, and illustrates probabilistic inference in Gen using a simple generic inference algorithm.\n",
    "\n",
    "This tutorial will guide you through how to:\n",
    "\n",
    "- Express a probabilistic model as a generative function in Gen\n",
    "\n",
    "- Obtain the trace of a generative function, and inspect and visualize it the trace.\n",
    "\n",
    "- Write a simple inference program based on a generic importance sampling inference algorithm.\n",
    "\n",
    "- Interpret the output of the inference program, and the effect of amount of computation on accuracy of inferences.\n",
    "\n",
    "The tutorial also illustrates different types of modeling flexibility afforded by the built-in modeling language. You will:\n",
    "\n",
    "- Write a probabilistic model that uses a stochastic if-else branch to infer which model of two models best explains a data set.\n",
    "\n",
    "- Write a probabilistic model that uses an unbounded number of parameters using recursion.\n",
    "\n",
    "Note that this tutorial does not cover *inference programming*, in which users implement inference algorithms that are specialized to their probabilistic model. Inference programming is important for getting accurate inferences efficiently, and will be covered in later tutorials. Also, this tutorial does not exhaustively cover all features of the modeling language -- there are also various features and extensions that provide improved performance that are not covered here.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Section 1.** [Julia, Gen, and this Jupyter notebook](#julia-gen-jupyter)\n",
    "\n",
    "**Section 2.** [Writing a probabilistic model as a generative function](#writing-model)\n",
    "\n",
    "**Section 3.** [Doing Bayesian inference](#doing-inference)\n",
    "\n",
    "**Section 4.** [Predicting new data](#predicting-data)\n",
    "\n",
    "**Section 5.** [Calling other generative functions](#calling-functions)\n",
    "\n",
    "**Section 6.** [Modeling with an infinite discrete hypothesis space](#infinite-space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Julia, Gen, and this Jupyter notebook  <a name=\"julia-gen-jupyter\"></a>\n",
    "\n",
    "Gen is a package for the Julia language. The package can be loaded with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gen programs typically consist of a combination of (i) probabilistic models written in modeling languages and (ii) inference programs written in regular Julia code. Gen provides a built-in modeling language that is itself based on Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses a Jupyter notebook. All cells in the notebook are regular Julia cells. Throughout the tutorial, we will use  that we use semicolons at the end of some cells so that the value of a cell is not printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 1 + 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the [PyPlot](https://github.com/JuliaPy/PyPlot.jl) Julia package for plotting. PyPlot wraps the matplotlib Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using PyPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will make use of Julia symbols. Note that a Julia symbol is different from a Julia string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(:foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(\"foo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing a probabilistic model as a generative function  <a name=\"writing-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic models represented in Gen as *generative functions*. The simplest way to construct a generative function is by using the built-in modeling DSL. Generative functions written in the built-in modeling DSL are based on Julia function definition syntax, but are prefixed with the `@gen` keyword. The generative function below represents a probabilistic model of a line in the x-y plane, and values of the y-coordinates associated with a given set of x-coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function line_model(xs::Vector{Float64})\n",
    "    n = length(xs)\n",
    "    slope = @addr(normal(0, 1), :slope)\n",
    "    intercept = @addr(normal(0, 2), :intercept)\n",
    "    for (i, x) in enumerate(xs)\n",
    "        @addr(normal(slope * x + intercept, 0.1), (:y, i))\n",
    "    end\n",
    "    return n\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative function takes as an argument a vector of x-coordinates. We create one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = [-5., -4., -3., -.2, -1., 0., 1., 2., 3., 4., 5.];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative function then samples a random choice representing the slope of a line from a normal distribution with mean 0 and standard deviation 1, and a random choice representing the intercept of a line from a normal distribution with mean 0 and standard deviation 2. In Bayesian statistics terms, these distributions are the *prior distributions* of the slope and intercept respectively. Then, the function samples values for the y-coordinates corresponding to each of the provided x-coordinates Each random choice has a unique *address*. A random choice is assigned an address using the `@addr` keyword. Addresses can be any Julia value. In this program, there are two types of addresses used -- Julia symbols and tuples of symbols and integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------\n",
    "### Exercise\n",
    "List the addresses of all random choices made when applying `line_model` to the vector `xs` defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------\n",
    "### Exercise\n",
    "\n",
    "Write a generative function that uses the same address twice. Run it to see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generative function returns the number of data points. We can run the function like we run a regular Julia function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = line_model(xs)\n",
    "println(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the random choices made by this generative function that are most important. The random choices are not included in the return value. They are however, included in the *trace* of the generative function. We can run the generative function and obtain its trace using the a method from the Gen API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trace, _) = Gen.initialize(line_model, (xs,));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method takes the function to be executed, and a tuple of arguments to the function, and returns a trace and a second value that we will not be using in this tutorial. When we print the trace, we see that it is a complex data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace contains various data about the execution. In particular, in contains the arguments on which the function was run, which are available with an API method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gen.get_args(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace also contains the value of the random choices, stored in map from addresses to their values. This map is also available through an API method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(Gen.get_assmt(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value is also recorded in the trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(Gen.get_retval(trace));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the probabilistic behavior of a generative function, it is helpful to be able to visualize the trace of a generative function. Below, we define a function that uses PyPlot to render a trace of the generative function above. The rendering shows the x-y data points and the line that is represented by the slope and intercept choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function render_trace(trace; show_data=true)\n",
    "    xs = get_args(trace)[1]\n",
    "    assmt = get_assmt(trace)\n",
    "    if show_data\n",
    "        ys = [assmt[(:y, i)] for i=1:length(xs)]\n",
    "        scatter(xs, ys, c=\"black\")\n",
    "    end\n",
    "    slope = assmt[:slope]\n",
    "    intercept = assmt[:intercept]\n",
    "    xmin = minimum(xs)\n",
    "    xmax = maximum(xs)\n",
    "    plot([xmin, xmax], slope *  [xmin, xmax] .+ intercept, color=\"black\", alpha=0.5)\n",
    "    ax = gca()\n",
    "    ax[:set_xlim]((xmin, xmax))\n",
    "    ax[:set_ylim]((xmin, xmax))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(3,3))\n",
    "render_trace(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because a generative function is stochastic, we need to visualize many runs in order to understand its behavoir. The cell below will allow us to render a grid of traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function grid(renderer, traces; ncols=6, nrows=3)\n",
    "    figure(figsize=(16, 8))\n",
    "    for (i, trace) in enumerate(traces)\n",
    "        subplot(nrows, ncols, i)\n",
    "        renderer(trace)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate several traces and render them in a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [initialize(line_model, (xs,))[1] for _=1:12]\n",
    "grid(render_trace, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "### Exercise\n",
    "\n",
    "Write a model that generates a sine wave of unknown phase, period and amplitude, and then generates y-coordinates from a given vector of x-coordinates by adding noise to the value of the wave at each x-coordinate.\n",
    "Use a Gamma distribution  (see [`Gen.gamma`](https://probcomp.github.io/Gen/dev/ref/distributions/#Gen.gamma)). for the prior distributions on the period and amplitude, and a uniform distribution for the phase (see [`Gen.uniform`](https://probcomp.github.io/Gen/dev/ref/distributions/#Gen.uniform)). Write a function that renders the trace by showing the data set and the sine wave. Visualize a grid of traces and discuss the distribution. Try tweaking the parameters of each of the prior distributions and seeing how the behavior changes.\n",
    "\n",
    "Hint: There should be three random choices corresponding to the period, amplitude, and phase, and then N random choices, one for each y-coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Doing Bayesian inference  <a name=\"doing-inference\"></a>\n",
    "\n",
    "We now will provide a data set of y-coordinates and try to draw inferences about the process that generated the data. We begin with the following data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys = [6.75003, 6.1568, 4.26414, 1.84894, 3.09686, 1.94026, 1.36411, -0.83959, -0.976, -1.93363, -2.91303];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(3,3))\n",
    "scatter(xs, ys, color=\"black\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by assuming that the line model was responsible for generatin the data, and inferring values of the slope and intercept that explain the data.\n",
    "\n",
    "To do this, we write a simple *inference program* that takes the model we are assuming, the data set, and the amount of computation to perform, and returns a trace of the function that is approximately sampled from the posterior distribution on traces of the function, given the observed data. This inference program is based on a Gen API method `importance_resampling`. Don't worry about the internals of this inference program yet. We will discuss inference programming in later tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function do_inference(model, xs, ys, amount_of_computation)\n",
    "    observations = Gen.DynamicAssignment()\n",
    "    for (i, y) in enumerate(ys)\n",
    "        observations[(:y, i)] = y\n",
    "    end\n",
    "    (trace, _) = Gen.importance_resampling(model, (xs,), observations, amount_of_computation);\n",
    "    return trace\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the inference program and visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = do_inference(line_model, xs, ys, 100)\n",
    "figure(figsize=(3,3))\n",
    "render_trace(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize many samples in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [do_inference(line_model, xs, ys, 100) for _=1:10];\n",
    "grid(render_trace, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in this case we can get a better sense for the variability in the posterior distribution by overlaying the traces. Each trace is going to have the same observed data points, so we only plot those once, based on the values in the first trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function overlay(renderer, traces; same_data=true, args...)\n",
    "    renderer(traces[1], show_data=true, args...)\n",
    "    for i=2:length(traces)\n",
    "        renderer(traces[i], show_data=!same_data, args...)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [do_inference(line_model, xs, ys, 100) for _=1:10];\n",
    "figure(figsize=(3,3))\n",
    "overlay(render_trace, traces);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "### Exercise\n",
    "\n",
    "The results above were obtained for `amount_of_computation = 100`. Run the algorithm with this value set to `1`, `10`, and `1000`, etc.  Which value seems like a good tradeoff between accuracy and running time? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "### Exercise\n",
    "Consider the following data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys_sine = [2.89, 2.22, -0.612, -0.522, -2.65, -0.133, 2.70, 2.77, 0.425, -2.11, -2.76];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(3, 3));\n",
    "scatter(xs, ys_sine, color=\"black\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write an inference program that generates traces of the sine wave model that explain this data set. Visualize the resulting distribution of traces. Experiment with a `gamma(1, 1)` and `gamma(5, 1)` prior on the period. Read about the Gamma distribution at https://en.wikipedia.org/wiki/Gamma_distribution. Discuss the results of inference? Do they make sense? How much computation did you need to get good results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predicting new data  <a name=\"predicting-data\"></a>\n",
    "\n",
    "By providing a third argument to `Gen.initialize`, it is possible to run a generatiev function with the values of certain random choices constrained to given values. The third argument an assignment. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = Gen.DynamicAssignment()\n",
    "constraints[:slope] = 0.\n",
    "constraints[:intercept] = 0.\n",
    "(trace, _) = Gen.initialize(line_model, (xs,), constraints)\n",
    "figure(figsize=(3,3))\n",
    "render_trace(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the random choices corresponding to the y-coordinates are still made randomly. Run the cell above a few times to verify this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the ability to run constrained executions of a generative function to predict the value of the y-coordinates at new x-coordinates by running new executions of the model generative function in which the random choices corresponding to the parameters have been constrained to their inferred values.  We have provided a function below that takes a trace, and a vector of new x-coordinates, and returns a vector of predicted y-coordinates corresponding to the x-coordinates in `new_xs`. We have designed this function to work with multiple models, so the set of parameter addresses is an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function predict_new_data(model, trace, new_xs::Vector{Float64}, param_addrs)\n",
    "    constraints = Gen.DynamicAssignment()\n",
    "    assmt = Gen.get_assmt(trace)\n",
    "    for addr in param_addrs\n",
    "        if Gen.has_value(assmt, addr)\n",
    "            constraints[addr] = assmt[addr]\n",
    "        end\n",
    "    end\n",
    "    (new_trace, _) = Gen.initialize(model, (new_xs,), constraints)\n",
    "    new_assmt = Gen.get_assmt(new_trace)\n",
    "    ys = [new_assmt[(:y, i)] for i=1:length(new_xs)]\n",
    "    return ys\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines a function that first performs inference on an observed data set `(xs, ys)`, and then runs `predict_new_data` to generate predicted y-coordinates. It repeats this process `num_traces` times, and returns a vector of the resulting y-coordinate vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function infer_and_predict(model, xs, ys, new_xs, param_addrs, num_traces, amount_of_computation)\n",
    "    pred_ys = []\n",
    "    for i=1:num_traces\n",
    "        trace = do_inference(model, xs, ys, amount_of_computation)\n",
    "        push!(pred_ys, predict_new_data(model, trace, new_xs, param_addrs))\n",
    "    end\n",
    "    pred_ys\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a cell that plots the observed data set `(xs, ys)` as red dots, and the predicted data as small black dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function plot_predictions(xs, ys, new_xs, pred_ys)\n",
    "    scatter(xs, ys, color=\"red\")\n",
    "    for pred_ys_single in pred_ys\n",
    "        scatter(new_xs, pred_ys_single, color=\"black\", s=1, alpha=0.3)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the original dataset for the line model. The x-coordinates span the interval -5 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(3,3))\n",
    "scatter(xs, ys, color=\"red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the inferred values of the parameters to predict y-coordinates for x-coordinates in the interval 5 to 10 from which data was not observed. We will also predict new data within the interval -5 to 5, and we will compare this data to the original observed data. Predicting new data from inferred parameters, and comparing this new data to the observed data is the core idea behind *posterior predictive checking*. This tutorial does not intend to give a rigorous overview behind techniques for checking the quality of a model, but intends to give high-level intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_xs = collect(range(-5, stop=10, length=100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate and plot the predicted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ys = infer_and_predict(line_model, xs, ys, new_xs, [:slope, :intercept], 20, 1000)\n",
    "figure(figsize=(3,3))\n",
    "plot_predictions(xs, ys, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look reasonable, both within the interval of observed data and in the extrapolated predictions on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the same experiment run with following data set, which has significantly more noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys_noisy = [5.092, 4.781, 2.46815, 1.23047, 0.903318, 1.11819, 2.10808, 1.09198, 0.0203789, -2.05068, 2.66031];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_ys = infer_and_predict(line_model, xs, ys_noisy, new_xs, [:slope, :intercept], 20, 1000)\n",
    "figure(figsize=(3,3))\n",
    "plot_predictions(xs, ys_noisy, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the generated data is less noisy than the observed data in the regime where data was observed, and it looks like the forecasted data is too overconfident. This is a sign that our model is mis-specified. In our case, this is because we have assumed that the noise has value 0.1. However, the actual noise in the data appears to be much larger. We can correct this by making the noise a random choice as well and inferring its value along with the other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first write a new version of the line model that samples a random choice for the noise from a `gamma(1, 1)` prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function line_model_2(xs::Vector{Float64})\n",
    "    n = length(xs)\n",
    "    slope = @addr(normal(0, 1), :slope)\n",
    "    intercept = @addr(normal(0, 2), :intercept)\n",
    "    noise = @addr(gamma(1, 1), :noise)\n",
    "    for (i, x) in enumerate(xs)\n",
    "        @addr(normal(slope * x + intercept, noise), (:y, i))\n",
    "    end\n",
    "    return nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we compare the predictions using inference the unmodified and modified model on the `ys` data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(6,3))\n",
    "subplot(1, 2, 1)\n",
    "pred_ys = infer_and_predict(line_model, xs, ys, new_xs, [:slope, :intercept], 20, 1000)\n",
    "plot_predictions(xs, ys, new_xs, pred_ys)\n",
    "subplot(1, 2, 2)\n",
    "pred_ys = infer_and_predict(line_model_2, xs, ys, new_xs, [:slope, :intercept, :noise], 20, 10000)\n",
    "plot_predictions(xs, ys, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is more uncertainty in the predictions made using the modified model.\n",
    "\n",
    "We also compare the predictions using inference the unmodified and modified model on the `ys_noisy` data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(6,3))\n",
    "subplot(1, 2, 1)\n",
    "pred_ys = infer_and_predict(line_model, xs, ys_noisy, new_xs, [:slope, :intercept], 20, 1000)\n",
    "plot_predictions(xs, ys_noisy, new_xs, pred_ys)\n",
    "subplot(1, 2, 2)\n",
    "pred_ys = infer_and_predict(line_model_2, xs, ys_noisy, new_xs, [:slope, :intercept, :noise], 20, 10000)\n",
    "plot_predictions(xs, ys_noisy, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that while the unmodified model was very overconfident, the modified model has an appropriate level of uncertainty, while still capturing the general negative trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "### Exercise\n",
    "\n",
    "Write a modified version the sine model that makes noise into a random choice. Compare the predicted data with the observed data `infer_and_predict` and `plot_predictions` for the unmodified and modified model, and for the `ys_sine` and `ys_noisy` datasets. Discuss the results. Experiment with the amount of inference computation used. The amount of inference computation will need to be higher for the model with the noise random choice. We have provided you with starter code. Convert these cells from `Raw NBConvert` cells to `Code` cells and fill them in."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@gen function sine_model_3(xs::Vector{Float64})\n",
    "    <fill in code here>\n",
    "    return nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "figure(figsize=(6,3))\n",
    "subplot(1, 2, 1)\n",
    "pred_ys = infer_and_predict(sine_model_2, xs, ys_sine, new_xs, <fill in code here>, 20, <fill in code here>)\n",
    "plot_predictions(xs, ys_sine, new_xs, pred_ys)\n",
    "subplot(1, 2, 2)\n",
    "pred_ys = infer_and_predict(sine_model_3, xs, ys_sine, new_xs, <fill in code here>, 20, <fill in code here>)\n",
    "plot_predictions(xs, ys_sine, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "figure(figsize=(6,3))\n",
    "subplot(1, 2, 1)\n",
    "pred_ys = infer_and_predict(sine_model_2, xs, ys_noisy, new_xs, <fill in code here>, 20, <fill in code here>)\n",
    "plot_predictions(xs, ys_noisy, new_xs, pred_ys)\n",
    "subplot(1, 2, 2)\n",
    "pred_ys = infer_and_predict(sine_model_3, xs, ys_noisy, new_xs, <fill in code here>, 20, <fill in code here>)\n",
    "plot_predictions(xs, ys_noisy, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calling other generative functions  <a name=\"calling-functions\"></a>\n",
    "\n",
    "In addition to making random choices, generative functions can invoke other generative functions. To illustrate this, we will write a probabilistic model that combines the line model and the sine model. This model is able to explain data using either model, and which model is chosen will depend on the data. This is called *model selection*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generative function can invoke another generative function in two ways -- using `@splice` or using `@addr`. When invoking using `@splice`, the random choices of the callee function are placed in the same address namespace as the caller's random choices. When using `@addr(<call>, <addr>)`, the random choices of the callee are placed under the namespace `<addr>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function foo()\n",
    "    @addr(normal(0, 1), :y)\n",
    "end\n",
    "\n",
    "@gen function bar_splice()\n",
    "    @addr(bernoulli(0.5), :x)\n",
    "    @splice(foo())\n",
    "end\n",
    "\n",
    "@gen function bar_addr()\n",
    "    @addr(bernoulli(0.5), :x)\n",
    "    @addr(foo(), :z)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first show the addresses sampled by `bar_splice`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trace, ) = initialize(bar_splice, ())\n",
    "println(get_assmt(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the addresses sampled by `bar_addr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trace, ) = initialize(bar_addr, ())\n",
    "println(get_assmt(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `@addr` instead of `@splice` can help avoid address collisions for complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we write a generative function that combies the line and sine models. It makes a Bernoulli random choice (e.g. a coin flip that returns true or false) that determines which of the two models will generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function combined_model(xs::Vector{Float64})\n",
    "    if @addr(bernoulli(0.5), :is_line)\n",
    "        @splice(line_model_2(xs))\n",
    "    else\n",
    "        @splice(sine_model_3(xs))\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also write a visualization for a trace of this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function render_combined(trace; show_data=true)\n",
    "    assmt = get_assmt(trace)\n",
    "    if assmt[:is_line]\n",
    "        render_trace(trace, show_data=show_data)\n",
    "    else\n",
    "        render_sine_trace(trace, show_data=show_data)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize some traces, and see that sometimes it samples linear data and other times sinusoidal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [initialize(combined_model, (xs,))[1] for _=1:12];\n",
    "grid(render_combined, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run inference using this combined model on the `ys` data set and the `ys_sine` data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(6,3))\n",
    "subplot(1, 2, 1)\n",
    "traces = [do_inference(combined_model, xs, ys, 10000) for _=1:10];\n",
    "overlay(render_combined, traces)\n",
    "subplot(1, 2, 2)\n",
    "traces = [do_inference(combined_model, xs, ys_sine, 10000) for _=1:10];\n",
    "overlay(render_combined, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Exercise \n",
    "\n",
    "There is code duplication in `line_model_3` and `sine_model_3`. Refactor the model to reduce code duplication and improve the readability of the code. Re-run the experiment above and confirm that the results are qualitatively the same. You may need to write a new rendering function. Try to avoid introducing code duplication between the model and the rendering code.\n",
    "\n",
    "Hint: To avoid introducing code duplication between the model and the rendering code, use the return value of the generative function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Construct a data set for which it is ambiguous whether the line or sine wave model is best. Visualize the inferred traces usingn `render_either` to illustrate the ambiguity. Write a program that takes the data set and returns an estimate of the posterior probability that the data was generated by the sine wave model, and run it on your data set.\n",
    "\n",
    "Hint: To estimate the posterior probability that the data was generated by the sine wave model, run the inference program many times to compute a large number of traces, and then compute the fraction of those traces in which `:is_line` is false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling with an infinite discrete hypothesis space  <a name=\"infinite-space\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gen's built-in modeling language can be used to express models that include more complex explanations for data that from hypothesis spaces that include an infinite set of possible discrete combinatorial structures. This section walks you through development of a model of data that does not a-priori specify an upper bound on the complexity model, but instead infers the complexity of the model as well as the parameters. This is a simple example of a *Bayesian nonparametric* model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider two data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs_dense = collect(range(-5, stop=5, length=50))\n",
    "ys_simple = fill(1., length(xs_dense)) .+ randn(length(xs_dense)) * 0.1\n",
    "ys_complex = [Int(floor(abs(x/3))) % 2 == 0 ? 2 : 0 for x in xs_dense] .+ randn(length(xs_dense)) * 0.1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(6,3))\n",
    "subplot(1, 2, 1)\n",
    "scatter(xs_dense, ys_simple, color=\"black\", s=10)\n",
    "gca()[:set_ylim]((-1, 3))\n",
    "subplot(1, 2, 2)\n",
    "scatter(xs_dense, ys_complex, color=\"black\", s=10);\n",
    "gca()[:set_ylim]((-1, 3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set on the left appears to be best explained as a contant function with some noise. The data set on the right appears to include four changepoints, with a constant function in between the changepoints. We want a model that does not a-priori choose the number of changepoints in the data. To do this, we will recursively partition the interval into regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "struct Interval\n",
    "    l::Float64\n",
    "    u::Float64\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract type Node end\n",
    "    \n",
    "struct InternalNode <: Node\n",
    "    left::Node\n",
    "    right::Node\n",
    "    interval::Interval\n",
    "end\n",
    "\n",
    "struct LeafNode <: Node\n",
    "    value::Float64\n",
    "    interval::Interval\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function generate_segments(l::Float64, u::Float64)\n",
    "    interval = Interval(l, u)\n",
    "    if @addr(bernoulli(0.7), :isleaf)\n",
    "        value = @addr(normal(0, 1), :value)\n",
    "        return LeafNode(value, interval)\n",
    "    else\n",
    "        frac = @addr(beta(2, 2), :frac)\n",
    "        mid  = l + (u - l) * frac\n",
    "        left = @addr(generate_segments(l, mid), :left)\n",
    "        right = @addr(generate_segments(mid, u), :right)\n",
    "        return InternalNode(left, right, interval)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function render_node(node::LeafNode)\n",
    "    plot([node.interval.l, node.interval.u], [node.value, node.value])\n",
    "end\n",
    "\n",
    "function render_node(node::InternalNode)\n",
    "    render_node(node.left)\n",
    "    render_node(node.right)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function render_segments_trace(trace)\n",
    "    node = get_retval(trace)\n",
    "    render_node(node)\n",
    "    ax = gca()\n",
    "    ax[:set_xlim]((0, 1))\n",
    "    ax[:set_ylim]((-3, 3))\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate 12 traces from this function and visualize them below. We plot the piecewise constant function that was sampled by each run of the generative function. Different constant segments are shown in different colors. Run the cell a few times to get a better sense of the distribution on functions that is represented by the generative function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [initialize(generate_segments, (0., 1.))[1] for i=1:12]\n",
    "grid(render_segments_trace, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generative function that generates an unknown partition into segments with values, we write a model that adds noise to the resulting constant functions to generate a data set of y-coordinates. The noise level will be a random choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function get_value_at(x::Float64, node::LeafNode)\n",
    "    @assert x >= node.interval.l && x <= node.interval.u\n",
    "    return node.value\n",
    "end\n",
    "\n",
    "function get_value_at(x::Float64, node::InternalNode)\n",
    "    @assert x >= node.interval.l && x <= node.interval.u\n",
    "    if x <= node.left.interval.u\n",
    "        get_value_at(x, node.left)\n",
    "    else\n",
    "        get_value_at(x, node.right)\n",
    "    end\n",
    "end\n",
    "\n",
    "@gen function changepoint_model(xs::Vector{Float64})\n",
    "    node = @addr(generate_segments(minimum(xs), maximum(xs)), :tree)\n",
    "    noise = @addr(gamma(1, 1), :noise)\n",
    "    for (i, x) in enumerate(xs)\n",
    "        @addr(normal(get_value_at(x, node), noise), (:y, i))\n",
    "    end\n",
    "    return node\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a visualization for `changepoint_model` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function render_cp_model_trace(trace; show_data=true)\n",
    "    xs = get_args(trace)[1]\n",
    "    node = get_retval(trace)\n",
    "    render_node(node)\n",
    "    assmt = get_assmt(trace)\n",
    "    if show_data\n",
    "        ys = [assmt[(:y, i)] for i=1:length(xs)]\n",
    "        scatter(xs, ys, c=\"black\")\n",
    "    end\n",
    "    ax = gca()\n",
    "    ax[:set_xlim]((minimum(xs), maximum(xs)))\n",
    "    ax[:set_ylim]((-3, 3))\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate some simulated data sets and visualize them on top of the underlying piecewise constant function from which they were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [initialize(changepoint_model, (xs_dense,))[1] for i=1:12]\n",
    "grid(render_cp_model_trace, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the amount of variability around the piecewise constant mean function differs from trace to trace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform inference for the simple data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [do_inference(changepoint_model, xs_dense, ys_simple, 10000) for _=1:12];\n",
    "grid(render_cp_model_trace, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we inferred that the mean function that explains the data is a constant with very high probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inference about the complex data set, we use more computation. You can experiment with different amounts of computation to see how the quality of the inferences degrade with less computation. Note that we are using a very simple generic inference algorithm in this tutorial. In later tutorials, we will learn how to write more efficient algorithms, so that accurate results can be obtained with significantly less amount of computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "traces = [do_inference(changepoint_model, xs_dense, ys_complex, 100000) for _=1:12];\n",
    "grid(render_cp_model_trace, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Exercise\n",
    "Write a function that plots the histogram of the probability distribution on the number of changepoints.\n",
    "Show the results for the `ys_simple` and `ys_complex` data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### Exercise\n",
    "Write a new version of `changepoint_model` that uses `@splice` to make the recursive calls instead of `@addr`.\n",
    "\n",
    "Hint: Recall that addresses can be arbitrary values, not just symbols."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
